---
title: "Severity Index Building"
author: "Technical documentation - UNHCR"
date: " `r format(Sys.Date(),  '%B %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true 
params:
  datafolder: "data-raw" ## This is the default folder where to put you data in
  data: "data_module-input.xlsx" ## Name of the data file
  shp: "gtm_admbnda_adm2_ocha_conred_20190207.shp" ## name of the shapefile to create the map
  max_winsorisation:  5
  skew_thresh:  2
  kurt_thresh: 3.5
  same_thresh: 0.5
  collin_thresh: 0.9
  neg_corr_thresh: -0.4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE,
                      collapse = FALSE,
                      comment = "#>",
                      # fig.width = 5.5, fig.height = 4,
                      fig.retina = 2, 
                      fig.width = 9,
                      fig.asp = 0.618,
                      fig.align = "center",
                      dev = "ragg_png",
                      out.width = "90%")
options(scipen = 999) # turn-off scientific notation like 1e+48
set.seed(1)
extrafont::loadfonts(quiet=TRUE)
library("ggplot2")
library("unhcrthemes")

# NOTE install dev version of COINr at
# pak::pkg_install("bluefoxr/COINr")
library("COINr")


# pak::pkg_install("martinepunhcr/MVI_Guatemala") 
library("BuildIndex")

# NOTE install iCOINr at 
# pak::pkg_install("bluefoxr/iCOINr")
library("iCOINr")


file_path = here::here(params$datafolder, params$data)
shp_path = here::here(params$datafolder, params$shp) 

## Comment the following 2 lines to test the template with your data - as per YAML parameter above!
# file_path = system.file("data_input/data_module-input.xlsx", package = "BuildIndex")
# file_path = system.file("data_input/gtm_admbnda_adm2_ocha_conred_20190207.shp", package = "BuildIndex")
```

# Introduction

This document records the construction of the Severity Index.

To be able to reach, properly assess and understand local dynamics, vulnerabilities and capacities of the displaced and host populations alike, humanitarian organisations are increasingly using sub-national [Area Based Approach](https://www.humanitarianlibrary.org/collection/implementing-area-based-approaches). Area based approaches define "*an area, rather than a sector or target group, as a primary entry point*". Such approach is particularly appropriate when residents in an affected area face complex, inter-related and multisectoral needs, resulting in risk of forced displacement. Severity index informs therfore comparison of needs across geographic areas. The challenge is to summarize and condense the information of a plurality of underlying indicators into a single measure, in a way that accurately reflects the underlying concept.

The approach used here generates Transparency, Robustness & Accuracy as it follows the standards [10 steps](https://knowledge4policy.ec.europa.eu/sites/default/files/10-step-pocket-guide-to-composite-indicators-and-scoreboards.pdf) in a fully reproducible approach, using [coinR package](https://bluefoxr.github.io/COINr/). It also ensures Credibility & Engagement through the effective participation of field experts as the last stage of the process.

# Conceptual Framework

The "theoretical" conceptual framework of the Severity Index is defined as shown in the figure below [NOTE: to ensure this is the final version?]:

![Theoretical conceptual framework](Marco%20conceptual.png)

However, based on the indicators available in the country and that have been used as the input here, the conceptual framework used in practice is shown in the interactive diagram below.

You can click on the diagram to drill down the different elements of the index.

```{r}
MVI <- f_data_input(file_path )
f_plot_framework(MVI)
```

This may differ from the theoretical version in cases where no indicators are available for certain categories or dimensions, in which case these are automatically removed.

Some further details summarising the input data are given below. These should be checked to ensure they agree with expectations.

```{r}

## it needs to come with a description of each column for proper understanding
f_print_coin(MVI)
```

# Indicator analysis

In this section indicators are analysed from a statistical perspective. Specifically the following analyses are run:

1. Univariate analysis based on data availability, uniqueness, and distribution properties.
2. Bivariate analysis based on correlations.

This is automated in a single function which flags any indicators that have "problems", based on the following criteria:

- Percentage missing data (indicators flagged with less than 66%)
- Percentage unique values (any indicators with greater than 50% observations with the same value are flagged)
- Presence of outliers, flagged by having both absolute skew > 2 and kurtosis > 3.5
- Collinearity with other indicators in the same aggregation group (defined as a rank correlation > 0.9)
- Strong negative correlation with other indicators in the same group (defined as a rank correlation < -0.4)

Running the analysis generates the following table:

```{r}
MVI <- f_analyse_indicators(MVI)
f_display_indicator_analysis(MVI)
```

In the table, only indicators with at least one "flag" based on the criteria mentioned are listed. In order to dig further into the flagged indicators, we can plot individual distributions: this can show the nature of outliers that are flagged.

```{r}

iCOINr::iplot_dist(MVI, iCode = "Refugiados")
```

To explore other indicators change `iCode` in the source code to the required indicator code. Note that outliers may point to an error, but are often simply a feature of the data. However it is important to check. In the next section, any distributions with outliers will be treated.

Similarly we can explore the relationships between indicators using scatter plots, and this helps to check any collinear or negatively-correlated indicators as flagged in the table.

```{r}
plot_scatter(MVI, dsets = "Raw", iCodes = c("Dormitoria", "VivCollectCalle"),
             log_scale = c(FALSE, TRUE)) + 
  theme(text = element_text(size = 25))
```

Again, this plot can be changed by changing the indicator codes.

If any indicators are highly problematic (e.g. multiple flags and/or very low data availability) they can be considered for removal using the `f_remove_indicators()` function.

# Index construction

In order to build the index from this point, we follow three steps:

1. Outlier treatment
2. Normalisation
3. Aggregation

In practice this is performed by a single dedicated function `f_build_index()`. Here, the steps followed inside this function are broken down.

**Outlier treatment** aims to adjust the distributions of highly skewed, or fat tailed indicators, including cases where there are outliers that are not characteristic of the rest of the distribution. This is done to improve the discriminatory power of the indicator in aggregation. For more on this, see [here](https://bluefoxr.github.io/COINrDoc/data-treatment.html).

To deal with this we follow a standard procedure which looks like this, for each indicator:

1. Check the **skew and kurtosis** of the indicator
2. If the absolute skew is greater than 2 AND the kurtosis is greater than 3.5, data treatment is applied (step 3 onwards), else leave the indicator as it is and move back to 1 for the next indicator.
3. Apply **Winsorisation** up to a maximum number of five points. Check after each Winsorised point whether skew and kurtosis fall back within the limits specified. If so, apply no further data treatment and move to the next indicator.
4. If the maximum number of Winsorised points is reached and skew and kurtosis are still outside the thresholds, "undo" any Winsorised points and apply a log transformation.

Running `f_build_index()` performs this outlier treatment procedure automatically to all indicators (any without outliers are skipped).

```{r}
MVI <- f_build_index(MVI)
```

The details of how each indicator was treated are shown in the following table.

```{r}
MVI$Analysis$Treated$Dets_Table |>
  tidy_treatment_details() |>
  filter(!Pass_0) |>
  DT::datatable(rownames = FALSE)
```

The next step in the construction process is to **normalise** the indicators. This is done by transforming each indicator onto a $[1,100]$ scale using a "min-max" transformation. The objective is to put indicators on the same scale for the purposes of balanced aggregation. This is a linear transformation as illustrated by a before/after plot of any given indicator:

```{r}
plot_scatter(MVI, dsets = c("Treated", "Normalised"), iCodes = "InsegAlim") + 
  theme(text = element_text(size = 25))
```

The final operation is to aggregate the indicators. In this document we consider several alternative aggregation methods representing various "scenarios" for further comparison by field experts. Therefore, details of each aggregation method are left until the following section. Note that the choice of which aggregation methods to include here is still up for discussion.

# Aggregation Options

Our first view of the results is as a results table. The table is sorted by default from the highest scoring (most vulnerable) municipalities downwards, based on the Index scores for different scenario.

## Scenario 1: Arithmetic mean

```{r}
MVI1 <- f_build_index(coin = MVI, 
                     agg_method = "a_amean", #   (arithmetic mean),
                     max_winsorisation = params$max_winsorisation, 
                     skew_thresh = params$skew_thresh , 
                     kurt_thresh = params$kurt_thresh )
```

```{r}
results_table <- COINr::get_results(MVI1, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```

These results should be checked to see whether they agree with common sense. Another way of looking at the results is in a bar chart. The colours within each bar represent the contributions to the index score from each Dimension. This bar chart shows the top 30 most vulnerable municipalities.

```{r}
# note: install latest version of COINr

plotbar <- COINr::plot_bar(MVI1, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE, 
                uLabel = "uName", ##--> Does not seems to work...   #  Error in codes2names(coin, iData$uCode) : 
                                  ## see comment above              # all(iCodes %in% iMeta$iCode) is not TRUE
                  #uLabel = "uCode",
                filter_to_ends = list(top = 30), flip_coords = TRUE) 

plotbar + theme(text = element_text(size = 25))
  
```

The final view of the results is as a choropleth map.

```{r}
f_plot_map(coin = MVI1, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r}
f_export_to_excel(coin = MVI1, 
                  fname = here::here("inst", "index_export_artih.xlsx"))
```

## Scenario 2: Geometric mean

One issue to address when aggregating indicators is related to the concept of compensability. Namely the question is to know to what extent can we accept that the high score of an indicator should compensate the low score of another indicator? While the arithmetic mean has "perfect compensability", the geometric mean only allows [partial compensation](https://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means) between indicator scores.

```{r}
MVI2 <- f_build_index(coin = MVI, 
                     agg_method = "a_gmean", # (geometric mean),
                     max_winsorisation = params$max_winsorisation, 
                     skew_thresh = params$skew_thresh , 
                     kurt_thresh = params$kurt_thresh )
```

```{r}
results_table <- COINr::get_results(MVI2, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```

Again the top 30 municipalities in this scenario:

```{r}
COINr::plot_bar(MVI2, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE,
                uLabel = "uName",
                filter_to_ends = list(top = 30), flip_coords = TRUE) + 
  theme(text = element_text(size = 25))
```

The map:

```{r}
f_plot_map(coin = MVI2, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r}
f_export_to_excel(coin = MVI2, 
                  fname = here::here("inst", "index_export_geo.xlsx"))
```

## Scenario 3: Benefit of the Doubt

This method is the application of Data Envelopment Analysis (DEA) to the field of composite indicators. It was originally proposed by Melyn and Moesen (1991) to evaluate macroeconomic performance. ACAPS has prepared an excellent note on The use of data envelopment analysis to [calculate priority scores in needs assessments](https://www.acaps.org/sites/acaps/files/resources/files/the_use_of_data_envelopment_analysis_to_calculate_priority_scores_in_needs_assessments_july_2015.pdf).

BoD approach offers several advantages:

-   Weights are endogenously determined by the observed performances and benchmark is not based on theoretical bounds, but it's a linear combination of the observed best performances.

-   Principle is easy to communicate: since we are not sure about the right weights, we look for "benefit of the doubt" weights (such that your overall relative performance index is as high as possible).

```{r}
MVI3 <- f_build_index(coin = MVI, 
                     agg_method = "a_bod", # (benefit of doubt via Compind package) 
                     max_winsorisation = params$max_winsorisation, 
                     skew_thresh = params$skew_thresh , 
                     kurt_thresh = params$kurt_thresh )
```

```{r}
results_table <- COINr::get_results(MVI3, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```

Again the top 30 municipalities in this scenario:

```{r}
COINr::plot_bar(MVI3, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE,
                uLabel = "uName",
                filter_to_ends = list(top = 30), flip_coords = TRUE) + 
  theme(text = element_text(size = 25))
```

The map:

```{r}
f_plot_map(coin = MVI3, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r, eval=FALSE}
f_export_to_excel(coin = MVI3, 
                  fname = here::here("inst", "index_export_bod.xlsx"))
```

## Scenario 4: Dendric method

Dendric method (also known as the Wroclaw Taxonomic Method ), originally developed at the University of Wroclaw, is based on the distance from a theoretical unit characterized by the best performance for all indicators considered.

The final composite indicator is therefore based on the sum of euclidean distances from the ideal unit and normalized by a measure of variability of these distance (`mean + 2*std`).

```{r}
MVI4 <- f_build_index(coin = MVI, 
                     agg_method = "a_wroclaw", # (Wroclaw Taxonomic Method via Compind)
                     max_winsorisation = 5,
                     skew_thresh = 2,
                     kurt_thresh = 3.5)

```

```{r}
results_table <- COINr::get_results(MVI4, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```

Again the top 30 municipalities in this scenario:

```{r}
COINr::plot_bar(MVI4, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE,
                uLabel = "uName",
                filter_to_ends = list(top = 30), flip_coords = TRUE) + 
  theme(text = element_text(size = 25))
```

The map:

```{r}
f_plot_map(coin = MVI4, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r, eval=FALSE}
f_export_to_excel(coin = MVI4, 
                  fname = here::here("inst", "index_export_wro.xlsx"))
```

# Comparison

We can compare the aggregation options as follows:

```{r}
df_compare <- COINr::compare_coins_multi(
  list(
    Arithmetic = MVI1,
    Geometric = MVI2,
    BOD = MVI3,
    Wroclaw = MVI4),
  dset = "Aggregated",
  iCode = "MVI",
  also_get = "uName"
)

DT::datatable(df_compare, rownames = F)
```

We can also plot any of the ranks of one result against another. For example: the arithmetic mean against the geometric mean.

```{r}
ggplot2::qplot(df_compare$Arithmetic, df_compare$Geometric,
               xlab = "Arithmetic", ylab = "Geometric")  + 
  theme(text = element_text(size = 25))
```

This demonstrates in this case the fairly large difference between the two methodologies.

# Uncertainty Analysis

Uncertainty analysis involves estimating the uncertainty in the the scores and ranks of the composite indicator, given the uncertainties in the methodological alternatives for the aggregation approach.

The results of an uncertainty include for example confidence intervals over the ranks and median ranks.

Here we will perform an uncertainty analysis which tests the effect of changing the aggregation method, and the weights. To do this we read in some weight sets from a spreadsheet that represent plausible alternatives. These are added to the "coin" and can be accessed during aggregation.

```{r}
library(readxl)

w_base <- MVI$Meta$Weights$Original
w_base <- w_base[w_base$Level == 1, ]

f_path <- here::here("inst", "data_input", "weights.xlsx")
# get names of tabs
w_names <- excel_sheets(f_path)


for(w_name in w_names){
  w <- read_excel(here::here("inst", "data_input", "weights.xlsx"), sheet = w_name) |>
    as.data.frame()
  # bind with indicator level
  w <- rbind(w[names(w_base)], w_base)
  # ensure any unused aggs removed
  w <- w[w$iCode %in% MVI$Meta$Ind$iCode, ]
  # add to coin
  MVI$Meta$Weights[[w_name]] <- w
}


```

We can now build the specifications for the uncertainty analysis.

```{r}
# component of SA_specs for weights
l_weights <- list(Address = "$Log$Aggregate$w",
                  Distribution = names(MVI$Meta$Weights),
                  Type = "discrete")

# component for aggregation
l_agg <- list(Address = "$Log$Aggregate$f_ag",
               Distribution = c("a_amean", "a_gmean"),
               Type = "discrete")

# create overall specification list
SA_specs <- list(
  Weights = l_weights,
  Aggregation = l_agg
)
```

Now we run the uncertainty analysis. Since this involves regenerating the results many times, it can take some time. Set `N` to a lower value to make this faster (but less accurate).

```{r}
SA_res <- get_sensitivity(MVI, SA_specs = SA_specs, N = 50, SA_type = "SA",
                          dset = "Aggregated", iCode = "MVI", Nboot = 100, check_addresses = FALSE)
```

Now we can plot the results. First, the uncertainty:

```{r}
plot_uncertainty(SA_res) + theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank()) + 
  theme(text = element_text(size = 25))
```

Here the vertical lines represent 90% confidence intervals for the rank of each unit based on the range of ranks observed due to the different combinations of weights and aggregation methods. It demonstrates that the range of uncertainty is quite wide. To investigate further, we can check the relative importance of the two assumptions via a sensitivity analysis:

```{r}
plot_sensitivity(SA_res, ptype = "box") + 
  theme(text = element_text(size = 25))
```

This shows that the aggregation method is the most important assumption here. However it is also true that the weight sets are rather different from one another.

The outcome is that the level of uncertainty is fairy high. However, this is quite typical when changing between arithmetic and geometric mean. It points to the need to ideally decide whether the concept of "severity" is better dealt with by the arithmetic mean or the geometric mean, because the choice is important. Notably, this analysis excludes the other two aggregation methods which would further inflate the uncertainty.

# Field experts "Reality Check"

Composite indicators try to capture complex multidimensional socioeconomic concepts in a single measure. Inevitably this involves making a number of assumptions in the construction process, and this introduces uncertainties.

As with any model, composite indicators should be validated if at all possible. In composite indicators, an important "validation" step is for field experts to carefully check the results to see to what extent they agree with intuition and experience. If the results seem unusual, it is important not to dismiss them but to try to understand why. Is the discrepancy due to the choice of methodology? Or the fact that some important indicators may be missing or not weighted strongly enough? Is the conceptual framework fit for purpose? Or, is this a genuine signal from the data that agrees with the concept and may be revealing something new to the experts? In short, it is important to explore the data carefully down to the indicator level, during the validation step, as opposed to looking only at the index level.

To summarise, the "Reality check" of the results with the Country Panel Expert can include asking:

- Do the results options make sense to field experts?
- If the results don't make sense, why not?
- Are there any big gaps in terms of indicators measured?
- Are there need to reshuffles of indicators/categories?

Generally speaking the objective is to improve the index, and to strike a balance between guiding the index towards the results you expect, without adjusting the methodology to only agree with prior assumptions. 

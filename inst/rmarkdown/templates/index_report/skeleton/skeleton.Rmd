---
title: "Severity Index Building"
author: "Technical documentation - UNHCR"
date: " `r format(Sys.Date(),  '%B %Y')`"
output:
  unhcrdown::html_page:
    toc: true
    toc_depth: 2
    toc_float: true 
params:
  datafolder: "data-raw" ## This is the default folder where to put you data in
  data: "data_module-input.xlsx" ## Name of the data file
  shp: "gtm_admbnda_adm2_ocha_conred_20190207.shp" ## name of the shapefile to create the map
  max_winsorisation:  5
  skew_thresh:  2
  kurt_thresh: 3.5
  same_thresh: 0.5
  collin_thresh: 0.9
  neg_corr_thresh: -0.4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE,
                      collapse = FALSE,
                      comment = "#>",
                      # fig.width = 5.5, fig.height = 4,
                      fig.retina = 2, 
                      fig.width = 9,
                      fig.asp = 0.618,
                      fig.align = "center",
                      dev = "ragg_png",
                      out.width = "90%")
options(scipen = 999) # turn-off scientific notation like 1e+48
set.seed(1)
extrafont::loadfonts(quiet=TRUE)
library("ggplot2")
library("unhcrthemes")
library("COINr")
library("BuildIndex")

file_path = here::here(params$datafolder, params$data)
shp_path = here::here(params$datafolder, params$shp) 

## Comment the following 2 lines to test the template with your data - as per YAML parameter above!
# file_path = system.file("data_input/data_module-input.xlsx", package = "BuildIndex")
# file_path = system.file("data_input/gtm_admbnda_adm2_ocha_conred_20190207.shp", package = "BuildIndex")
```


# Introduction

This document records the construction of the Severity Index.

To be able to reach, properly assess and understand local dynamics, vulnerabilities and capacities of the displaced and host populations alike, humanitarian organisations are increasingly using sub-national [Area Based Approach](https://www.humanitarianlibrary.org/collection/implementing-area-based-approaches). Area based approaches define “_an area, rather than a sector or target group, as a primary entry point_”. Such approach is particularly appropriate when residents in an affected area face complex, inter-related and multisectoral needs, resulting in risk of forced displacement. Severity index informs therfore comparison of needs across geographic areas.  The challenge is to summarize and condense the information of a plurality of underlying indicators into a single measure, in a way that accurately reflects the underlying concept.

The approach used here generates Transparency, Robustness & Accuracy as it follows the standards [10 steps](https://knowledge4policy.ec.europa.eu/sites/default/files/10-step-pocket-guide-to-composite-indicators-and-scoreboards.pdf) in a fully reproducible approach, using [coinR package](https://bluefoxr.github.io/COINr/). It also ensures Credibility & Engagement through the effective participation of field experts as the last stage of the process.


# Conceptual Framework

Based on available indicators in the country, the conceptual framework can be represented in the interactive diagram below.

You can click on the diagram to drill down the different elements of the index. 

```{r}
MVI <- f_data_input(file_path )
f_plot_framework(MVI)
```


In details, the composite indicators can be described below:

```{r}

## it needs to come with a description of each column for proper understanding
f_print_coin(MVI)
```


# Data treatment

For convenience the main steps (Data treatment; Normalisation and Aggregation) have now been condensed down and simplified to a function that look at:
 
 
 * __Outlier treatment__ : Outlier treatment aims to adjust the distributions of highly skewed, or fat tailed indicators, including cases where there are outliers that are not characteristic of the rest of the distribution. This is done to improve the discriminatory power of the indicator in aggregation. For more on this, see [here](https://bluefoxr.github.io/COINrDoc/data-treatment.html).

 * __skew and kurtosis__ : If the absolute skew is greater than 2 AND the kurtosis is greater than 3.5, data treatment is applied (step 3 onwards), else leave the indicator as it is and move back to 1 for the next indicator.

 * __Winsorisation__ :  up to a maximum number of five points. Check after each Winsorised point whether skew and kurtosis fall back within the limits specified. If so, apply no further data treatment and move to the next indicator. If the maximum number of Winsorised points is reached and skew and kurtosis are still outside the thresholds, "undo" any Winsorised points and apply a log transformation.


A number of indicators might require data treatment. To deal with this we follow a standard procedure is built as a default
 

```{r}
MVI <- f_analyse_indicators(MVI)
f_display_indicator_analysis(MVI)
```

Now Building the index

```{r}
MVI <- f_build_index(MVI)

## Too many column to dislay
f_display_results_table(MVI)  

# results_table <- COINr::get_results(MVI, 
#                                     dset = "Treated",
#                              tab_type = "Summ",
#                              also_get = "uName", 
#                              nround = 1) 
# 
# results_table |>
#   dplyr::select(1:7) |>
#   DT::datatable() |>
#   DT::formatStyle(columns = ncol(results_table), fontSize = '50%')

```

We can see that most indicators have been dealt with by applying a log transformation as expected, whereas a few have been Winsorised. In total, after treatment four indicators still fall outside the skew/kurtosis limits. We will check these visually:

```{r}
outlying_indicators <- MVI$Analysis$Treated$Dets_Table$iCode[!MVI$Analysis$Treated$Dets_Table$check_SkewKurt2.Pass] |>
  na.omit()

plot <- COINr::plot_dist(MVI, 
                 dset = "Treated", 
                 iCodes = outlying_indicators, 
                 type = "Dot") 
plot <- plot + unhcrthemes::theme_unhcr()

plot 
```

This shows a problem: that one of the indicators is unusually negatively skewed. In this case, applying a log transformation won't work because that corrects for positive skew. To deal with this I have encoded a function in COINr which can deal with negative skew as well, and this is invoked here. In fact, it checks the direction of skew and applies the correct transformation.

```{r}
MVI <- qTreat(MVI, 
              dset = "Raw", 
              winmax = params$max_winsorisation, 
              skew_thresh = params$skew_thresh , 
              kurt_thresh = params$kurt_thresh, 
              f2 = "log_CT_plus")
```

Now let's check the outcome. We just focus on "AccElectr" here which is the problematic indicator:

```{r}
p1 <- COINr::plot_dist(MVI, 
                       dset = "Treated", 
                       iCodes = "AccElectr", 
                       type = "Dot")
p2 <- COINr::plot_scatter(MVI, 
                          dsets = c("Raw", "Treated"), 
                          iCodes = "AccElectr")

library(patchwork)
p1 + p2
```

This demonstrates the effectiveness of the new transformation: it has normalised the indicator but retaining its ordering. The scale of the indicator is now different (as with all transformations) but this is not important since indicators will anyway be scaled between 0-100 in the following step, and the scaling and transformation is only for the purposes of aggregation. When presenting individual indicators, we will of course present the real data.

 * __Normalise__ : Following this we can normalise the indicators using a standard per default min-max approach. This scales each indicator onto the $[0,100]$ interval.


 * __Aggregate__ : Now we create aggregate levels by aggregating up to the index. Different options will be used and generated for further comparison by field experts
 
 
 

# Aggregation Options

Our first view of the results is as a results table. The table is sorted by default from the highest scoring (most vulnerable) municipalities downwards, based on the Index scores for different scenario.

## Scenario 1: Arithmetic mean

```{r}
MVI1 <- f_build_index(coin = MVI, 
                     agg_method = "a_amean", #   (arithmetic mean),
                     max_winsorisation = params$max_winsorisation, 
                     skew_thresh = params$skew_thresh , 
                     kurt_thresh = params$kurt_thresh )
```



```{r}
results_table <- COINr::get_results(MVI1, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```


These results should be checked to see whether they agree with common sense. Another way of looking at the results is in a bar chart. Here, since we have a lot of municipalities I will just plot the top thirty. They are coloured by departamento.

```{r}
# COINr::plot_bar(MVI1, 
#                 dset = "Aggregated", 
#                 iCode = "MVI", 
#                ##  by_group = "Departamento", 
#                 filter_to_ends = list(top = 30))
```

We can plot the same chart but broken down by Dimension scores - this can give a view of how much each dimension contributes to the total score.

```{r}

plotbar <- COINr::plot_bar(MVI1, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE, 
              #  uLabel = "uName", ##--> Does not seems to work...   #  Error in codes2names(coin, iData$uCode) : 
                                                                     # all(iCodes %in% iMeta$iCode) is not TRUE
                  uLabel = "uCode",
                filter_to_ends = list(bottom = 30)) 

plotbar <- plotbar +
  coord_flip()

plotbar
  
```

As a last view of the results (for the moment), we can plot a choropleth map. This is based on the municipal shape files.

```{r}
f_plot_map(coin = MVI1, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r}
f_export_to_excel(coin = MVI1, 
                  fname = here::here("inst", "index_export_artih.xlsx"))
```


## Scenario 2: Geometric mean


One issue to address when aggregating indicators is related to the concept of compensability. Namely the question is to know to what extent can we accept that the high score of an indicator go to compensate the low score of another indicator?  

Geometric aggregation allows to bypass the full compensability hypothesis.

```{r}
MVI2 <- f_build_index(coin = MVI, 
                     agg_method = "a_gmean", # (geometric mean),
                     max_winsorisation = params$max_winsorisation, 
                     skew_thresh = params$skew_thresh , 
                     kurt_thresh = params$kurt_thresh )
```

```{r}
results_table <- COINr::get_results(MVI2, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```


These results should be checked to see whether they agree with common sense. Another way of looking at the results is in a bar chart. Here, since we have a lot of municipalities I will just plot the top thirty. They are coloured by departamento.

```{r}
# COINr::plot_bar(MVI2, 
#                 dset = "Aggregated", 
#                 iCode = "MVI", 
#                #  by_group = "Departamento", 
#                 filter_to_ends = list(top = 30))
```

We can plot the same chart but broken down by Dimension scores - this can give a view of how much each dimension contributes to the total score.

```{r}
COINr::plot_bar(MVI2, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE, 
                filter_to_ends = list(bottom = 30))
```

As a last view of the results (for the moment), we can plot a choropleth map. This is based on the municipal shape files.

```{r}
f_plot_map(coin = MVI2, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r}
f_export_to_excel(coin = MVI2, 
                  fname = here::here("inst", "index_export_geo.xlsx"))
```

## Scenario 3: Benefit of the Doubt


This method is the application of Data Envelopment Analysis (DEA) to the field of composite indicators. It was originally proposed by Melyn and Moesen (1991) to evaluate macroeconomic performance. ACAPS has prepared an excellent note on The use of data envelopment analysis to [calculate priority scores in needs assessments](https://www.acaps.org/sites/acaps/files/resources/files/the_use_of_data_envelopment_analysis_to_calculate_priority_scores_in_needs_assessments_july_2015.pdf).

BoD approach offers several advantages:

 *  Weights are endogenously determined by the observed performances and benchmark is not based on theoretical bounds, but it’s a linear combination of the observed best performances.

 *  Principle is easy to communicate: since we are not sure about the right weights, we look for ”benefit of the doubt” weights (such that your overall relative performance index is as high as possible).



```{r}
MVI3 <- f_build_index(coin = MVI, 
                     agg_method = "a_bod", # (benefit of doubt via Compind package) 
                     max_winsorisation = params$max_winsorisation, 
                     skew_thresh = params$skew_thresh , 
                     kurt_thresh = params$kurt_thresh )
```


```{r}
results_table <- COINr::get_results(MVI3, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```


These results should be checked to see whether they agree with common sense. Another way of looking at the results is in a bar chart. Here, since we have a lot of municipalities I will just plot the top thirty. They are coloured by departamento.

```{r}
# COINr::plot_bar(MVI3, 
#                 dset = "Aggregated", 
#                 iCode = "MVI", 
#                #  by_group = "Departamento", 
#                 filter_to_ends = list(top = 30))
```

We can plot the same chart but broken down by Dimension scores - this can give a view of how much each dimension contributes to the total score.

```{r}
COINr::plot_bar(MVI3, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE, 
                filter_to_ends = list(bottom = 30))
```

As a last view of the results (for the moment), we can plot a choropleth map. This is based on the municipal shape files.

```{r}
f_plot_map(coin = MVI3, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r}
# f_export_to_excel(coin = MVI3, 
#                   fname = here::here("inst", "index_export_bod.xlsx"))
```

## Scenario 4: Dendric method

Dendric method (also known as the Wroclaw Taxonomic Method ), originally developed at the University of Wroclaw, is based on the distance from a theoretical unit characterized by the best performance for all indicators considered.

The final composite indicator is therefore based on the sum of euclidean distances from the ideal unit and normalized by a measure of variability of these distance (`mean + 2*std`).


```{r}
MVI4 <- f_build_index(coin = MVI, 
                     agg_method = "a_wroclaw", # (Wroclaw Taxonomic Method via Compind)
                     max_winsorisation = 5,
                     skew_thresh = 2,
                     kurt_thresh = 3.5)

```

```{r}
results_table <- COINr::get_results(MVI4, dset = "Aggregated",
                             tab_type = "Aggs",
                             also_get = "uName", 
                             nround = 1) 

results_table |>
  dplyr::select(1:7) |>
  DT::datatable() |>
  DT::formatStyle(columns = ncol(results_table), fontSize = '50%')
```


These results should be checked to see whether they agree with common sense. Another way of looking at the results is in a bar chart. Here, since we have a lot of municipalities I will just plot the top thirty. They are coloured by departamento.

```{r}
# COINr::plot_bar(MVI4, 
#                 dset = "Aggregated", 
#                 iCode = "MVI", 
#                #  by_group = "Departamento", 
#                 filter_to_ends = list(top = 30))
```

We can plot the same chart but broken down by Dimension scores - this can give a view of how much each dimension contributes to the total score.

```{r}
COINr::plot_bar(MVI4, 
                dset = "Aggregated", 
                iCode = "MVI", 
                stack_children = TRUE, 
                filter_to_ends = list(bottom = 30))
```

As a last view of the results (for the moment), we can plot a choropleth map. This is based on the municipal shape files.

```{r}
f_plot_map(coin = MVI4, 
           dset = "Aggregated",
           iCode = "MVI", 
           shp_path  )
```

```{r}
# f_export_to_excel(coin = MVI4, 
#                   fname = here::here("inst", "index_export_wro.xlsx"))
```



# Uncertainty Analysis

Uncertainty analysis involves estimating the uncertainty in the the scores and ranks of the composite indicator, given the uncertainties in the methodological alternatives for the aggregation approach. 

The results of an uncertainty include for example confidence intervals over the ranks and median ranks.


# Field experts "Reality Check" 


"Reality check" of the results with the Country Panel Expert can be facilitated as a last step: 

 *  Do the results options make sense to field experts?

 *  Are there any big gaps in terms of indicators measured? 

 *  Are there need to reshuffles of indicators/categories ?


